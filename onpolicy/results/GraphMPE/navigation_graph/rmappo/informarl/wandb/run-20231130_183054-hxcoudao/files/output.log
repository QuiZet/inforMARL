Overriding Observation dimension
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0-1): 2 x TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0-1): 2 x TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
/home/yungisimon/anaconda3/envs/hetnet/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Average episode rewards is -83.452 	Total timesteps: 3200 	 Percentage complete 0.160
Traceback (most recent call last):
  File "/home/yungisimon/inforMARL/onpolicy/scripts/train_mpe.py", line 315, in <module>
    main(sys.argv[1:])
  File "/home/yungisimon/inforMARL/onpolicy/scripts/train_mpe.py", line 300, in main
    runner.run()
  File "/home/yungisimon/inforMARL/onpolicy/runner/shared/graph_mpe_runner.py", line 76, in run
    train_infos = self.train()
  File "/home/yungisimon/inforMARL/onpolicy/runner/shared/base_runner.py", line 168, in train
    train_infos = self.trainer.train(self.buffer)
  File "/home/yungisimon/inforMARL/onpolicy/algorithms/graph_mappo.py", line 329, in train
    ) = self.ppo_update(sample, update_actor)
  File "/home/yungisimon/inforMARL/onpolicy/algorithms/graph_mappo.py", line 170, in ppo_update
    values, action_log_probs, dist_entropy = self.policy.evaluate_actions(
  File "/home/yungisimon/inforMARL/onpolicy/algorithms/graph_MAPPOPolicy.py", line 267, in evaluate_actions
    values, _ = self.critic.forward(
  File "/home/yungisimon/inforMARL/onpolicy/algorithms/graph_actor_critic.py", line 399, in forward
    nbd_features = self.gnn_base(
  File "/home/yungisimon/anaconda3/envs/hetnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yungisimon/inforMARL/onpolicy/algorithms/utils/gnn.py", line 486, in forward
    x = self.gnn(node_obs, adj, agent_id)
  File "/home/yungisimon/anaconda3/envs/hetnet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yungisimon/inforMARL/onpolicy/algorithms/utils/gnn.py", line 257, in forward
    edge_index, edge_attr = self.processAdj(adj[i])
  File "/home/yungisimon/inforMARL/onpolicy/algorithms/utils/gnn.py", line 331, in processAdj
    assert adj.dim() >= 2 and adj.dim() <= 3
KeyboardInterrupt